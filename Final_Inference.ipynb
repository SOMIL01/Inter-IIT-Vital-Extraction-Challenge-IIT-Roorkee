{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmKJJPIGgPmf"
      },
      "source": [
        "**Importing Libraries** <br>\n",
        "Import all the necessary GitHub Repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEMegBuQnLO2"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!git clone https://github.com/PaddlePaddle/PaddleOCR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQQQD02W4IsA"
      },
      "source": [
        "pip intsall all the necessary Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyysSsdVnA3-"
      },
      "outputs": [],
      "source": [
        "!python -m pip install paddlepaddle -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install paddleocr\n",
        "\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykdSaMzc4kj5"
      },
      "source": [
        "Import all the modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQAsizkggN7I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from IPython.display import Image #this is to render predictions\n",
        "\n",
        "from PIL import Image\n",
        "import numpy\n",
        "import timeit\n",
        "import threading\n",
        "\n",
        "from paddleocr import PaddleOCR, draw_ocr # main OCR dependencies\n",
        "from matplotlib import pyplot as plt # plot images\n",
        "import cv2 #opencv\n",
        "import os # folder directory navigation\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4DGjH98f2oI"
      },
      "source": [
        "**Loading Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "_WW0MsO3fzV4"
      },
      "outputs": [],
      "source": [
        "model_screen = torch.hub.load('/content/yolov5', 'custom', path='./weights/bestm.pt', source='local',_verbose=False)\n",
        "model_vitals = torch.hub.load('/content/yolov5', 'custom', path='./weights/best_vital.pt', source='local',_verbose=False)\n",
        "model_hrGraph = torch.hub.load('/content/yolov5', 'custom', path='./weights/best_hr.pt', source='local',_verbose=False)\n",
        "model_garbage = torch.hub.load('/content/yolov5', 'custom', path='./weights/best_garbage.pt', source='local',_verbose=False)\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='en',show_log = False,debug=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QKXkU2AfP16"
      },
      "source": [
        "**Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "id": "QQASjcJafOW3"
      },
      "outputs": [],
      "source": [
        "def removing_multiple_labelling(res,num_classes):\n",
        "  res_dup = res.xyxy[0]\n",
        "  arr = []\n",
        "  for i in range(0,num_classes):\n",
        "    arr.append([])\n",
        "  res_list = res_dup.tolist()\n",
        "  for i in range(0,len(res_list)):\n",
        "    for j in range(0,num_classes):\n",
        "      if(int(res_list[i][-1] )== j):\n",
        "        if(len(arr[j]) == 0):\n",
        "            arr[j] = res_list[i]\n",
        "        else:\n",
        "            if(arr[j][-2] < res_list[i][-2]):\n",
        "              arr[j] = res_list[i]\n",
        "  arr_copy = []\n",
        "  for aj in arr:\n",
        "    if(len(aj) != 0):\n",
        "      arr_copy.append(aj)\n",
        "\n",
        "  arr_copy_tensor = torch.tensor(arr_copy)\n",
        "  return arr_copy_tensor\n",
        "\n",
        "dict_vitals = {\n",
        "    0:\"DBP\",\n",
        "    1:\"HR\",\n",
        "    2: \"MAP\",\n",
        "    3: \"RR\",\n",
        "    4: \"SBP\",\n",
        "    5: \"SPO2\"\n",
        "}\n",
        "\n",
        "def final_graph_func(image):\n",
        "  \n",
        "  img_np = numpy.asarray(image)\n",
        "\n",
        "  #Using the model weights to detect garbage texts from the image\n",
        "  results = model_garbage(image)\n",
        "  res_dup = results.xyxy[0]\n",
        "\n",
        "  img_gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  def get_mask(image,res_dup):\n",
        "    mask=image\n",
        "    dh, dw = image.shape\n",
        "    for dt in res_dup:\n",
        "      # Split string to float\n",
        "      x1, y1, x2, y2 = dt[:-2]\n",
        "      x1=int(x1)\n",
        "      x2=int(x2)\n",
        "      y1=int(y1)\n",
        "      y2=int(y2)\n",
        "      for j in range(y1,y2):\n",
        "        for i in range(x1,x2):\n",
        "          mask[j][i]=0\n",
        "    return mask\n",
        "\n",
        "  # Calling the masking function to obtain the image with black masks on the garbage regions\n",
        "  masked_img=get_mask(img_gray, res_dup)\n",
        "\n",
        "  # Binarizing the image \n",
        "  ret, thresh = cv2.threshold(masked_img, 200, 255, cv2.THRESH_BINARY)\n",
        "  \n",
        "  # Post processing of the binarized image to obtain contours and plotting the same.\n",
        "  def final_processing(image):\n",
        "    #Extracts all contours\n",
        "    #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "    edged = cv2.Canny(blurred, 20, 150)\n",
        "    contours, _ = cv2.findContours(edged, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  \n",
        "    data_points = []\n",
        "    final_img=numpy.full(image.shape,255)\n",
        "    for contour in contours:\n",
        "      for data in contour:\n",
        "        data_points.append((data[0][0],data[0][1]))\n",
        "\n",
        "    for data in data_points:\n",
        "      final_img[int(data[1]),int(data[0])]=0\n",
        "\n",
        "    # Plot the image with grid lines\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(final_img)\n",
        "    ax.grid(which='both', color='orange', linewidth=0.05)\n",
        "    ax.plot()\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    cv2_imshow(final_img)\n",
        "    return final_img\n",
        "  \n",
        "  final_processing(thresh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-pqjNb4e4-2"
      },
      "source": [
        "**Screen Extraction using YOLOv5m6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {
        "id": "ULBCv3XQe3FE"
      },
      "outputs": [],
      "source": [
        "def screen_extraction(image_path:str):\n",
        "  im1 = Image.open(image_path)\n",
        "  results=model_screen(im1, size=640)\n",
        "  results.xyxy[0] = removing_multiple_labelling(results,1)\n",
        "  cropped = im1.crop(results.xyxy[0][0][:-2].numpy())\n",
        "  return cropped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsX4FJVeiJQm"
      },
      "source": [
        "**Vitals Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "lxsC-VSOiOQ7"
      },
      "outputs": [],
      "source": [
        "def number_extraction(crop):\n",
        "  result = ocr.ocr(crop, cls=True)\n",
        "  if(len(result[0]) == 0):\n",
        "    return -1\n",
        "  result_list = result[0]\n",
        "  arr_tuple = []\n",
        "  for i in result_list:\n",
        "    arr_tuple.append(i[-1])\n",
        "  arr_tuple.sort(reverse = True, key = lambda t: t[-1])\n",
        "  text = arr_tuple[0][0]\n",
        "  num = \"\"\n",
        "  for i in text:\n",
        "    if i.isdigit():\n",
        "      num += i\n",
        "  if num == \"\":\n",
        "    return -1\n",
        "  return int(num)\n",
        "\n",
        "def vitals_extraction(cropped):\n",
        "  res=model_vitals(cropped, size=640)\n",
        "  res_dup = removing_multiple_labelling(res,6)\n",
        "  ret_dict = {}\n",
        "  for labels in res_dup:\n",
        "    cropped_copy = cropped\n",
        "    crop_save = cropped_copy.crop(labels[:-2].numpy())\n",
        "    #converting B/W and paddig\n",
        "    cropped_list = numpy.asarray(crop_save)\n",
        "    cropped_list=cv2.cvtColor(cropped_list, cv2.COLOR_BGR2GRAY)\n",
        "    cropped_list = cv2.copyMakeBorder(cropped_list, 50,50,50,50,cv2.BORDER_CONSTANT)\n",
        "    classes = dict_vitals[int(labels[-1])]\n",
        "    value = number_extraction(cropped_list)\n",
        "    if(value != -1 and classes != \"MAP\"):\n",
        "      ret_dict[classes] = value\n",
        "  return ret_dict"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EsyexaKm6vrz"
      },
      "source": [
        "**HR Curve Digitization** <br> call graph_extraction() with argument as the image path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {
        "id": "b3oFLD-I61wo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Extracts the graph from the image of the screen to be used as input for the final_graph_func function \n",
        "def graph_extraction(image_path:str):\n",
        "  screen = screen_extraction(image_path)\n",
        "  res = model_hrGraph(screen, size=640)\n",
        "  res_dup = removing_multiple_labelling(res,1)\n",
        "  cropped = screen.crop(res_dup[0][:-2].numpy())\n",
        "  final_graph_func(cropped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVaO85Hsy-7m"
      },
      "source": [
        "## FINAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "I-KjjZInZd4q"
      },
      "outputs": [],
      "source": [
        "def inference(image_path:str):\n",
        "  '''\n",
        "  Function responsible for inference.\n",
        "  Args: \n",
        "    image_path: str, path to image file. eg. \"input/aveksha_micu_mon--209_2023_1_17_12_0_34.jpeg\"\n",
        "  Returns:\n",
        "    result: dict, final output dictionary. eg. {\"HR\":\"80\", \"SPO2\":\"98\", \"RR\":\"15\", \"SBP\":\"126\", \"DBP\":\"86\"}\n",
        "  '''\n",
        "  result = {}\n",
        "  screen = screen_extraction(image_path)\n",
        "  result = vitals_extraction(screen)\n",
        "  ### put your code here\n",
        "  \n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRUmLMW5VTT"
      },
      "source": [
        "We used this accuracy funtion to get final Accuracy (same as instructed in FAQ doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "m0WGvN4OJlQs"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred,gt):\n",
        "  ans = 0.0\n",
        "  for i in range(0,6):\n",
        "    if(i == 2):\n",
        "      continue\n",
        "    key = dict_vitals[i]\n",
        "    if((key in gt) and (key in pred)):\n",
        "      if (gt[key]==pred[key]):\n",
        "        ans += 0.2\n",
        "    if((not(key in gt)) and (not(key in pred))):\n",
        "      ans += 0.2\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmJyemqd9gTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inter IIT Tech Meet 11.0\n",
        "*Team ID: 17*\n",
        "\n",
        "### Abstract:\n",
        "Monitoring vitals is an important aspect of providing high-quality care to patients. With the increasing use of technology in healthcare, there are now many digital monitoring systems available that can help to automate the process of tracking vitals, making it more efficient and accurate.\n",
        "\n",
        "This challenge is aimed at extracting a patient's vitals like SpO2, RR, Systolic Blood Pressure, Diastolic Blood Pressure and MAP from the provided ECG monitor images. We have dealt with this problem in a sequence of three separate subproblems - extracting the screen from the initial image, performing object detection and recognising the text to obtain vitals. We have employed state of the art models, data augmentation techniques, noise reduction models on images, supervised and semi-supervised learning on the data given to obtain a fast and accurate network that takes in an image and outputs the patient's vitals.\n",
        "\n",
        "### Approach:\n",
        "We have attempted to solve this problem statement by dividing the core task into several sub tasks and targeting each part as a standalone problem. Our baseline model consists of the following steps :\n",
        "\n",
        "- **Screen Extraction** - Extracting only the relevant part of the image i.e the monitor. We experimented with several techniques and finally settled with YOLOv5m6.\n",
        "\n",
        "- **Object Detection to extract vitals** - After experimenting with several architectures we used YOLOv5m6 to identify which number on the screen corresponded to which vital.\n",
        "\n",
        "- **Character Recognition to obtain measurements** - With the help of Optical Character Recognition(OCR), we read the text contained within the bounding boxes given as output by YOLO.\n",
        "\n",
        "The above 3 steps lay down the most basic approach we had. Along with this we have later **mentioned our approach for detecting the H.R graph** and digitizing it.\n",
        "\n",
        "### Dataset :\n",
        "The dataset we obtained was in three parts:\n",
        "\n",
        "- **Monitor Segmentation Dataset** - The dataset consists of 2000 images having the segmentation boundaries for the monitors in each image .\n",
        "\n",
        "- **Classification Dataset** - The second dataset consists of a 1000 monitor images with each monitor belonging to one of the four classes present.\n",
        "\n",
        "- **Unlabelled Dataset** - The third dataset consists of 7000 unlabelled images with monitors from several different types of classes.\n",
        "\n",
        "### Baseline Model  :\n",
        "We train a baseline model to extract the measurements relating to each vital from the monitor image. Our **baseline model** consists of:\n",
        "\n",
        "- A **ResNet-18** based regressor to obtain the coordinates of the monitors' corners (i.e. to generate a bounding box)\n",
        "\n",
        "- Followed by **ResNet-18** to perform classification and determine the monitor type.\n",
        "\n",
        "- This is followed by object detection using **YOLOv5m6** to create bounding boxes around the different vital measurements like Heart Rate, SpO2, RR, MAP etc.\n",
        "\n",
        "- Finally **Paddle-Paddle OCR** reads the text inside the bounding boxes generated by YOLO and gives the final output.\n",
        "\n",
        "### Improvements on the Baseline:\n",
        "We undertook thorough literature reviews and conducted several experiments on our initial baseline by eliminating and adding parts,trying out different models for the subtasks and performing semi-supervised learning. Finally we settled on the following architecture as it gave the best performance.\n",
        "\n",
        "### Final architecture:\n",
        "- Screen Extraction via YOLOv5m6\n",
        "- Vital Sign extraction using YOLOv5m6\n",
        "- Text recognition using Paddle-Paddle OCR.\n",
        "\n",
        "### Why did we omit classification?\n",
        "Since the labeled data given to us was classified into 4 types, we naturally thought of using a CNN based classifier like ResNet to determine the image type and then training a separate YOLO network on each type for higher accuracy. However, since YOLO learns contextual information, it was able to detect the correct vitals irrespective of the monitor type and removal of the classifier lead to a significant decrease in latency while not affecting the accuracy negatively in any way.\n",
        "\n",
        "### Pipeline :\n",
        "\n",
        "![Pipeline](./images/flowchart.png)\n",
        "\n",
        "### Screen Extraction:\n",
        "\n",
        "- Our first approach to solve this problem was using ResNet-50 to predict the 4 corner points of the quadrilateral tracing the monitor by regression. However, the results were highly inaccurate on unseen data which led us to try out Instance Segmentation of the image by using Mask-RCNN.\n",
        " \n",
        "- Mask-RCNN outputs a bounding box around the monitor along with a pixel by pixel mask. The results after performing semi-supervised learning on the given data were highly accurate. The main issue we faced was an unusually high inference time per image(9 seconds when using Google Colab-CPU).\n",
        " \n",
        "- **YOLO** is extremely fast because it does not deal with complex pipelines. It sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Thus we performed training with YOLOv5m6 and got highly accurate results with a significant improvement in the inference time - **0.6 seconds approximately per image** , 1500% faster than Mask-RCNN.\n",
        "\n",
        "- We use **YOLOv5 – YOLOv5m6** pretrained on the COCO dataset.\n",
        "\n",
        "For improving accuracy and a more robust screen extraction, we employ **data augmentations** to learn the given images better.\n",
        "- We use vertical and horizontal flipping of the images\n",
        " \n",
        "- Changing the image saturation(between -25% and 25%).\n",
        "\n",
        "- We use Gaussian Blur to blur the images to 6px and also add random noise to 5% of pixels.\n",
        "\n",
        "![Screen](./images/screen.png)\n",
        "\n",
        "### YOLO's outputs for screen segmentation.\n",
        "\n",
        "### Object detection/extracting vitals :\n",
        "- Our first approach to solve this problem was exploring the different operations available and performing literature reviews of the said options. We read up about FasterRCNN, ResNet and YOLO and tried to understand how these models would analyze our data.\n",
        "\n",
        "- After thorough reading and reasoning, we chose YOLO to extract vital signs because YOLO can learn contextual information better than other object detection models.It also has the obvious advantages of better speed and accuracy.\n",
        "\n",
        "### Semi Supervised Learning :\n",
        "- We were given a small labeled dataset of 1000 images and unlabelled dataset with 7000 images. Thus the most natural thing to do was to carry out semi-supervised learning of YOLO on both the datasets. We chose **Pseudo-Labelling** , a very simple approach to semi supervised learning.\n",
        "\n",
        "- The model is initially trained on the labeled dataset and is used to make predictions for the unlabeled data. It selects the examples from the unlabeled dataset where the prediction is confident and considers its prediction as pseudo-label. This pseudo labeled dataset is then added to the labeled dataset and the model is then trained again on the expanded labeled dataset.\n",
        "\n",
        "- We developed a function to manipulate our text files such that we chose only the most confident predictions of each class. We then concatenated these predicted labels to the original dataset and retrained our network thereby implementing SSL with pseudo labeling.\n",
        "\n",
        "![vitals](./images/vitals.png)\n",
        "\n",
        "### Extracting Vital Signs using YOLOv5m6\n",
        "\n",
        "### Text extraction:\n",
        "- Once the outputs from YOLO are obtained, the only task left is to recognize the numbers within the bounding boxes.\n",
        "\n",
        "- We first implemented EasyOCR, a built-in python library which gave quite unsatisfactory results, so we trained and tested TesseractOCR. The problem with Tesseract is that it has too many modes,making it difficult to identify small and blurry text.\n",
        "\n",
        "- **PaddlePaddle** has many benefits over the other architectures mentioned above. PaddlePaddle OCR has state-of-the-art recognition accuracy. It works better than Tesseract when images are in RGB **/** BGR if the image cannot be binarised.It is based on the PaddlePaddle Platform, which is extremely fast and makes our model useful for real time application.\n",
        "\n",
        "- We first convert the RGB image to Grayscale. To adjust for very small bounding boxes generated by YOLO during Vital Sign Extraction, we added a padding of 50 pixels. This made it possible for PaddlePaddleOCR to detect the text easily.\n",
        "\n",
        "### H.R Curve Detection and Digitisation :\n",
        "- The task of extracting the curve from the image surprisingly turned out to be more complicated than anticipated. The bounding boxes for the HR graphs contain a lot of text on and around the actual curve making it pretty hard to extract out only the curve.\n",
        "\n",
        "- Performing edge detection and contour tracing on the image did not give satisfactory results.\n",
        "\n",
        "- We then tried to devise an **algorithm** : a kernel would scan the surrounding pixels to the right and follow the direction of the highest intensity pixels, thus essentially scanning a single pixel thick heart rate curve. However, this algorithm had too big of a time complexity to be used in real time and the problem of overlapping text and discontinuity in the curve made the problem too complex.\n",
        "\n",
        "- **Finally** we built a network that consists of a noise reducing-network followed by a series of steps to convert the image of the curve to a plot.\n",
        "\n",
        "1. First we train a YOLOv5s model to learn any noise in the image. For this we manually annotated the H.R. curves with bounding boxes containing any noise near the curve .\n",
        "\n",
        "2. We use the predictions by the aforementioned YOLO network to remove the garbage portions of the image(noise and unnecessary text) by blacking out all the predicted bounding boxes(changing pixel values to 0).\n",
        "\n",
        "3. We then binarize the image, and use contour plotting.\n",
        "\n",
        "4. Finally we sample the obtained image to get a series of points that is displayed as a plot.\n",
        "\n",
        "### Hyperparameters:\n",
        "\n",
        "**Yolov5** :\n",
        "\n",
        "- Adam optimiser with a weight decay of 0.0005 and momentum of 0.937.\n",
        "- initial learning rate of 0.01 and final OneCycleLR learning rate of 0.01.\n",
        "- We used the cosine LR scheduler with maximum learning rate set to the initial learning rate of 0.01 and T\\_max set to 50.\n",
        "- Our batch size was 32 for the entire pipeline.\n",
        "\n",
        "# Results\n",
        "<br> </br>\n",
        "![loss_curves](./images/loss_curves.jpeg)\n",
        "<br> </br>\n",
        "| **Technique** | **Accuracy (in %)** | **Inference time per image (in seconds)** |\n",
        "| --- | --- | --- |\n",
        "| Screen Extraction with Mask-RCNN → single YOLOv5 network for vitals  | 90.21 | 11.57  |\n",
        "| **Screen Extraction with YOLOv5m6 → 2 separate YOLOv5 networks for vitals** | **93.723** | **2.26**  |\n",
        "\n",
        "<br>\n",
        "\n",
        "Our final pipeline gave a 93.723% accuracy when tested on a randomly chosen set of 1000 images from the given unlabelled dataset, which was manually annotated. The model had an average inference time of 2.26 seconds per image(a 412% increase in speed from the Mask-RCNN based pipeline) \n",
        "\n",
        "## References:\n",
        "[Mask -RCNN](https://arxiv.org/abs/1703.06870)\n",
        "<br> </br>\n",
        "[YOLO](https://arxiv.org/abs/1506.02640)\n",
        "<br> </br>\n",
        "[PaddlePaddle OCR](https://arxiv.org/abs/2009.09941)\n",
        "<br> </br>\n",
        "[Resnet](https://arxiv.org/abs/1512.03385)\n",
        "<br> </br>\n",
        "[An Overview of Deep Semi-Supervised Learning](https://arxiv.org/abs/2006.05278)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "10b62b8a8a753aa434078e7dd0e75fabe451f71f178392c8e5a5f38079e8f0ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
